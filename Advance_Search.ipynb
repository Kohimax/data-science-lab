{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advance Search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T1Bteadxttc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "import collections\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np \n",
        "import string\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import reuters\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from wordcloud import WordCloud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyCSqlGTALWo",
        "colab_type": "code",
        "outputId": "9e0e2f46-8c99-4753-eb63-e12f6d36f5cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "porter=PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KbDpeN1vWXH",
        "colab_type": "code",
        "outputId": "1a0e6b61-f19c-4a2a-e1b8-62a80acf2de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bLi8q7rAgrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read Excel data as Data Frame\n",
        "def readExcelToDataFrame(path):\n",
        "    research_dataframe = pd.read_csv(path,index_col=False)\n",
        "    research_dataframe.drop(research_dataframe.columns[research_dataframe.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "    return research_dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ief3ZYlkAXPo",
        "colab_type": "code",
        "outputId": "eadf3659-7d4e-4840-9bb2-4a466cee0d14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "research_dataframe = readExcelToDataFrame('/content/drive/My Drive/Colab Notebooks/kaggle_covid_19.csv')\n",
        "research_dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>source</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>text_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>850dc8add6efb76a6dc341cb7a5a236c1dd3da7c</td>\n",
              "      <td>BIORXIV</td>\n",
              "      <td>Expanded skin virome in DOCK8-deficient patients</td>\n",
              "      <td>NaN</td>\n",
              "      <td>recurrent cutaneous and systemic infections, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dd6c1a719bd75ba7fb7c62291a676b39c99bb0bc</td>\n",
              "      <td>BIORXIV</td>\n",
              "      <td>Veterinary Science Molecular characterization ...</td>\n",
              "      <td>The cDNA nucleotide sequence of genome segment...</td>\n",
              "      <td>Members of the family Birnaviridae have 2-segm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>e88710cd7b238794bd75f21d308bc5442ad9713f</td>\n",
              "      <td>BIORXIV</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The common cold is one of the most prevalent a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d46d2f01a570bfb6e3b2e65ec50972d1649b6a3d</td>\n",
              "      <td>BIORXIV</td>\n",
              "      <td>Severe Acute Respiratory Syndrome: Lessons fro...</td>\n",
              "      <td>An outbreak of severe acute respiratory syndro...</td>\n",
              "      <td>A n outbreak of severe acute respiratory syndr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     doc_id  ...                                          text_body\n",
              "0                                       NaN  ...                                                NaN\n",
              "1  850dc8add6efb76a6dc341cb7a5a236c1dd3da7c  ...  recurrent cutaneous and systemic infections, a...\n",
              "2  dd6c1a719bd75ba7fb7c62291a676b39c99bb0bc  ...  Members of the family Birnaviridae have 2-segm...\n",
              "3  e88710cd7b238794bd75f21d308bc5442ad9713f  ...  The common cold is one of the most prevalent a...\n",
              "4  d46d2f01a570bfb6e3b2e65ec50972d1649b6a3d  ...  A n outbreak of severe acute respiratory syndr...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piuiCN8_RbqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_data = readExcelToDataFrame('/content/drive/My Drive/Colab Notebooks/metadata.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB_6pOJVRrkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanEmptyData(columnName,df):\n",
        "    return df[df[columnName].notnull()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOgxRM2iRwJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "research_dataframe = cleanEmptyData('text_body',research_dataframe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyi1YMGFSDv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_data[\"doc_id\"] = meta_data[\"sha\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlC8vEMkSKNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta_data = cleanEmptyData('doc_id', meta_data)\n",
        "meta_data = cleanEmptyData('publish_time', meta_data)\n",
        "meta_data = meta_data[meta_data['publish_time'].str.contains('2019') | meta_data['publish_time'].str.contains('2020')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIHqbD9tSiRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "research_dataframe = cleanEmptyData('doc_id', research_dataframe)\n",
        "research_dataframe = cleanEmptyData('text_body', research_dataframe)\n",
        "df  = research_dataframe.merge(meta_data, on='doc_id', how='right')\n",
        "df = df.drop([\"source\", \"abstract_x\", \"cord_uid\", \"abstract_x\",\"sha\",\"source_x\",\"title_y\",\"pmcid\",\"pubmed_id\",\"license\",\"abstract_y\",\"journal\",\"Microsoft Academic Paper ID\",\"WHO #Covidence\",\"has_pdf_parse\",\"has_pmc_xml_parse\",\"full_text_file\"], axis=1)\n",
        "\n",
        "df = cleanEmptyData('text_body', df)\n",
        "df = df.rename(columns={'title_x': 'title'})\n",
        "columns = [\"doc_id\",\"doi\", \"publish_time\", \"authors\",\"url\",\"title\", \"text_body\"]\n",
        "df = df.reindex(columns=columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4073EmQXOYc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"/content/drive/My Drive/Colab Notebooks/df_24042020.pkl\", \"wb\")\n",
        "pickle.dump(df, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_3SSnq9L8q9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = df['text_body']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDb2cpS6Ma8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = target_text.replace(np.nan, '', regex=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydqJJTFwMPZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sentence(data):\n",
        "  data = data.replace(np.nan, '', regex=True)\n",
        "  ## ! Check without removing the '\\n' Keep original sentences and tokenize\n",
        "  return [sent_tokenize(target_text[i]) for i in range(len(target_text)) if len(target_text[i]) != 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhLDLj4ByHxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = prepare_sentence(target_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8FZuDC1EWbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save sentences file\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/sentences_24042020.pkl\",\"wb\")\n",
        "pickle.dump(sentences, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn-Gxmom17jL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = [x.replace('\\n', ' ') for x in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx8QJ53X1_TT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = [re.sub(r'[^a-zA-Z]', ' ', str(x)) for x in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_XM2qqV2CZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = [re.sub(r'\\b[a-zA-Z]\\b', '', str(x)) for x in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQnm4uXO2DNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = [\"\".join(j for j in i if j not in string.punctuation) for i in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN-I1FjL2DZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = [word_tokenize(doc) for doc in set(target_text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r1pQ4St2DiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = [[word.lower() for word in doc] for doc in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxalPeHH2K36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyePQxPT2PYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = [[word for word in doc if word not in stop_words] for doc in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWVjzrIp2Phk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer() \n",
        "target_text = [[lemmatizer.lemmatize(word) for word in doc] for doc in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYn3NRbJ2PoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_of_words = Counter(word for doc in target_text for word in set(doc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKP-JZnT2PsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " num_of_words_sorted = [(l,k) for k,l in sorted([(j,i) for i,j in num_of_words.items()], reverse=True)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBzOBlNv2Pma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_low_freq = [word[0] for word in num_of_words_sorted if word[1] == 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lrcBjuR2Zlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_low_freq = set(words_low_freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ax_j_rI2Z6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = [[word for word in doc if word not in words_low_freq] for doc in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJLqncSHsVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_cleaning(target_text):\n",
        "  ## Clean text\n",
        "  ## Replace '\\n' by ' '\n",
        "  target_text = [x.replace('\\n', ' ') for x in target_text]\n",
        "  # Keep characters only\n",
        "  target_text = [re.sub(r'[^a-zA-Z]', ' ', str(x)) for x in target_text]\n",
        "  # Remove single characters\n",
        "  target_text = [re.sub(r'\\b[a-zA-Z]\\b', '', str(x)) for x in target_text]\n",
        "  # Remove punctuation (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\n",
        "  target_text = [\"\".join(j for j in i if j not in string.punctuation) for i in target_text]\n",
        "  ## Tokenize words\n",
        "  target_text = [word_tokenize(doc) for doc in set(target_text)]\n",
        "  ## Lower case words for all docs\n",
        "  target_text = [[word.lower() for word in doc] for doc in target_text]\n",
        "  ## Remove stop words from all docs\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  target_text = [[word for word in doc if word not in stop_words] for doc in target_text]\n",
        "  ## lemmatization\n",
        "  lemmatizer = WordNetLemmatizer() \n",
        "  target_text = [[lemmatizer.lemmatize(word) for word in doc] for doc in target_text]\n",
        "  ## Remove words that occur only once in all documents\n",
        "  # Check frequency of words and sort from high to low\n",
        "  num_of_words = Counter(word for doc in target_text for word in set(doc))\n",
        "  # num_of_words_sorted = OrderedDict(num_of_words.most_common())\n",
        "  num_of_words_sorted = [(l,k) for k,l in sorted([(j,i) for i,j in num_of_words.items()], reverse=True)]\n",
        "  # All words with a frequency of 1 (word[0] is a word and word[1] the frequency)\n",
        "  words_low_freq = [word[0] for word in num_of_words_sorted if word[1] == 1]\n",
        "  # Set to increase speed\n",
        "  words_low_freq = set(words_low_freq)\n",
        "  # Remove words with a frequency of 1 (this takes a while) = this takes too much time\n",
        "  target_text = [[word for word in doc if word not in words_low_freq] for doc in target_text]\n",
        "  return target_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzaoT6r02BQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDQQTvGlG8sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_text = data_cleaning(target_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDfGQ44gJ8_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save target_text file\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/target_data_24042020.pkl\", \"wb\")\n",
        "pickle.dump(target_text, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9QYugHtKbW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================================================================\n",
        "# PART IV: Vectorize (and calculate TF-IDF)\n",
        "# ============================================================================\n",
        "texts_flattened = [\" \".join(x) for x in target_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk54zss0K98x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Include with token_pattern also single characters\n",
        "vectorizer = TfidfVectorizer(lowercase=False, stop_words=None, token_pattern=r\"(?u)\\b\\w+\\b\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_VH_785P1Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = vectorizer.fit_transform(texts_flattened)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WxfWe2DK3cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_names = vectorizer.get_feature_names()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxF9PZGTP_pG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dense = vectors.todense()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EzE4y39mKoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Dictionary of unique words as values\n",
        "word2idx = dict(zip(feature_names, range(len(feature_names))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfopEbeEo32i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save word2idx file\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/word2idx_24042020.pkl\", \"wb\")\n",
        "pickle.dump(word2idx, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB3I_e1wpmVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary with the unique words as keys\n",
        "idx2word = {v:k for k,v in word2idx.items()}\n",
        "\n",
        "## Save idx2word file\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/idx2word_24042020.pkl\", \"wb\")\n",
        "pickle.dump(idx2word, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NWeLrfQp363",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## word2idx all feature_names \n",
        "features_names_num = [word2idx[feature] for feature in feature_names]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg07dKeLp8hT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "## Calculate tfidf\n",
        "# df_tfidf = pd.DataFrame(dense, columns=feature_names)\n",
        "df_tfidf = pd.DataFrame(dense, columns=features_names_num)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY3fLycZp_JV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## word2idx for all words in plot_data\n",
        "target_text = [[word2idx.get(word) for word in line] for line in target_text]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwN6WEEhqMSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save plot_data_num file\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/plot_data_24042020.pkl\", \"wb\")\n",
        "pickle.dump(target_text, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0STBjBtstRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dictionary with a list as values\n",
        "worddic = defaultdict(list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itt4HAl2tCQG",
        "colab_type": "code",
        "outputId": "bd6f8b3c-5c56-4268-dab6-58fb92f00c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "[worddic[word].append([target_text.index(doc), \n",
        "                        [index for index, w in enumerate(doc) if w == word], \n",
        "                        df_tfidf.loc[i, word]]) \n",
        "                        for i,doc in enumerate(target_text) for word in set(doc)]\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "471.6404128074646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqlwPj1mCEjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save pickle file for worddic_all\n",
        "f = open(\"/content/drive/My Drive/Colab Notebooks/worddic_all_24042020.pkl\",\"wb\")\n",
        "pickle.dump(worddic,f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKZh6RWVEVi1",
        "colab_type": "text"
      },
      "source": [
        "**Data Loading **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HfMO45NrxWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load pickle file sentences\n",
        "pickle_in = open('/content/drive/My Drive/Colab Notebooks/sentences_24042020.pkl', 'rb')\n",
        "sentences = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQYRkMP4r8lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load pickle file idx2word\n",
        "pickle_in = open('/content/drive/My Drive/Colab Notebooks/idx2word_24042020.pkl', 'rb')\n",
        "idx2word = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiM0Xo-WsP3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load pickle file word2idx\n",
        "pickle_in = open('/content/drive/My Drive/Colab Notebooks/word2idx_24042020.pkl', 'rb')\n",
        "word2idx = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQWjbjW4EPPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load pickle file worddic (numeric version)\n",
        "pickle_in = open('/content/drive/My Drive/Colab Notebooks/worddic_all_24042020.pkl', 'rb')\n",
        "worddic = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc22rgx9TnUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## data set\n",
        "pickle_in = open('/content/drive/My Drive/Colab Notebooks/df_24042020.pkl', 'rb')\n",
        "df = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKH16DLNTxEi",
        "colab_type": "code",
        "outputId": "244722f4-69fc-49bc-e800-a26d6873ee67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>doi</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>authors</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43d2d4072d40a3964ea330342b9222376d700531</td>\n",
              "      <td>10.3201/eid2509.181937</td>\n",
              "      <td>2019-09-10</td>\n",
              "      <td>Jing, Shuping; Zhang, Jing; Cao, Mengchan; Liu...</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>Lassa fever in travelers from West Africa</td>\n",
              "      <td>We identified a case of fatal acute respirator...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cf692fd14d468eaca2c460132802463f0747411e</td>\n",
              "      <td>10.3201/eid2503.171702</td>\n",
              "      <td>2019-03-10</td>\n",
              "      <td>Farag, Elmoubasher Abu Baker; Nour, Mohamed; E...</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>Survey on Implementation of One Health Approac...</td>\n",
              "      <td>H uman infections with Middle East respiratory...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4f2102dfde801cb0cab691992bbc61cbd98b1ca5</td>\n",
              "      <td>10.3201/eid2502.180798</td>\n",
              "      <td>2019-02-10</td>\n",
              "      <td>Rampling, Tommy; Page, Mark; Horby, Peter</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Although several hundred centers contribute to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45e91042ac20319b849f63a7dd00dda7d11de638</td>\n",
              "      <td>10.3201/eid2601.ac2601</td>\n",
              "      <td>2020-01-10</td>\n",
              "      <td>Breedlove, Byron</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>Hunters Searching among Starry Nights and at t...</td>\n",
              "      <td>Director of Astrobiology at Columbia Universit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d84f1c9036052ca0d9f8216cac4c5118ad8cea19</td>\n",
              "      <td>10.3201/eid2512.191002</td>\n",
              "      <td>2019-12-10</td>\n",
              "      <td>Stoian, Ana M.M.; Zimmerman, Jeff; Ji, Ju; Hef...</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>However, half-life calculations were based on ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     doc_id  ...                                          text_body\n",
              "0  43d2d4072d40a3964ea330342b9222376d700531  ...  We identified a case of fatal acute respirator...\n",
              "1  cf692fd14d468eaca2c460132802463f0747411e  ...  H uman infections with Middle East respiratory...\n",
              "2  4f2102dfde801cb0cab691992bbc61cbd98b1ca5  ...  Although several hundred centers contribute to...\n",
              "3  45e91042ac20319b849f63a7dd00dda7d11de638  ...  Director of Astrobiology at Columbia Universit...\n",
              "4  d84f1c9036052ca0d9f8216cac4c5118ad8cea19  ...  However, half-life calculations were based on ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4hMo-66E8D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create word search which takes multiple words (search sentence) \n",
        "# and finds documents that contain these words along with metrics for ranking:\n",
        "\n",
        "# Output: \n",
        "# searchsentence, words, fullcount_order, combocount_order, fullidf_order, fdic_order\n",
        "# (1) searchsentence: original sentence to be searched\n",
        "# (2) words: words of the search sentence that are found in the dictionary (worddic)\n",
        "# (3) fullcount_order: number of occurences of search words\n",
        "# (4) combocount_order: percentage of search terms\n",
        "# (5) fullidf_order: sum of TD-IDF scores for search words (in ascending order)\n",
        "# (6) fdic_order: exact match bonus: word ordering score\n",
        "\n",
        "# >>> example on limited dataset (first three docs of biorxiv))\n",
        "# search('Full-genome phylogenetic analysis')\n",
        "# (1) ('full-genome phylogenetic analysis',  \n",
        "# searchsentence: original search sentence\n",
        "# (2) ['phylogenetic', 'analysis'], \n",
        "# words: two of the search words are in the dictionary worddic\n",
        "# (3) [(1, 7), (0, 1)], \n",
        "# fullcount_order: the search words (as found in dict) \n",
        "# occur in total 7 times in doc 1 and 1 time in doc 0\n",
        "# (4) [(1, 1.0), (0, 0.5)], \n",
        "# combocount_order: max value is 1, \n",
        "# in doc 1 all searchwords (as in dict) are present (1), \n",
        "# in doc 0 only 1 of the 2 search words are present (0.5)\n",
        "# (5) [(1, 0.0025220519886750533), (0, 0.0005167452472220973)], \n",
        "# fullidf_order: \n",
        "# doc 1 has a total (sum) tf-idf of 0.0025220519886750533, \n",
        "# doc 0 a total tf-idf of 0.0005167452472220973\n",
        "# (6) [(1, 1)]) \n",
        "# fdic_order: doc 1 has once two search words next to each other\n",
        "# <<<\n",
        "\n",
        "def search(searchsentence, must_have_word=None):\n",
        "    # split sentence into individual words\n",
        "    searchsentence = searchsentence.lower()\n",
        "    # split sentence in words\n",
        "    words = word_tokenize(searchsentence)\n",
        "    # remove duplicates in search words\n",
        "    words = list(set(words))\n",
        "\n",
        "    # lemmatize search words and must_have_word\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    if must_have_word != None:\n",
        "        must_have_word = lemmatizer.lemmatize(must_have_word)\n",
        "\n",
        "    # add must_have_word (first position) to search words \n",
        "    # if not yet in search words and if in dictionary:\n",
        "    if must_have_word != None and (must_have_word not in words)\\\n",
        "            and (must_have_word in word2idx):     \n",
        "            words.insert(0, must_have_word)\n",
        "\n",
        "    # keep characters as in worddic\n",
        "    words = [re.sub(r'[^a-zA-Z]', ' ', str(w)) for w in words]\n",
        "    \n",
        "    # lemmatize search words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    \n",
        "    # remove words if not in worddic \n",
        "    # keep only the words that are in the dictionary\n",
        "    words = [word for word in words if word in word2idx.keys()]\n",
        "    numwords = len(words)\n",
        "    \n",
        "    # word2idx all words\n",
        "    words = [word2idx[word] for word in words]\n",
        "\n",
        "    # Subset of worddic with only search words\n",
        "    worddic_sub = {key: worddic[key] for key in words}\n",
        "    \n",
        "    # Subset of worddic with only search words\n",
        "    worddic_sub = {key: worddic[key] for key in words}\n",
        "    \n",
        "    # temp dictionaries\n",
        "    enddic = {}\n",
        "    idfdic = {}\n",
        "    closedic = {}\n",
        "\n",
        "\n",
        "    ## metrics fullcount_order and fullidf_order: \n",
        "    # sum of number of occurences of all words in each doc (fullcount_order) \n",
        "    # and sum of TF-IDF score (fullidf_order)\n",
        "    for word in words:\n",
        "        # print(word)\n",
        "        for indpos in worddic[word]:\n",
        "            # print(indpos)\n",
        "            index = indpos[0]\n",
        "            amount = len(indpos[1])\n",
        "            idfscore = indpos[2]\n",
        "            # check if the index is already in the dictionary: add values to the keys\n",
        "            if index in enddic.keys():\n",
        "                enddic[index] += amount\n",
        "                idfdic[index] += idfscore\n",
        "            # if not, just make a two new keys and store the values\n",
        "            else:\n",
        "                enddic[index] = amount\n",
        "                idfdic[index] = idfscore\n",
        "    fullcount_order = sorted(enddic.items(), key=lambda x: x[1], reverse=True)\n",
        "    fullidf_order = sorted(idfdic.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    ## metric combocount_order: \n",
        "    # percentage of search words (as in dict) that appear in each doc\n",
        "    words_docs = defaultdict(list)\n",
        "    # get for each word the docs which it is in\n",
        "    for k in worddic_sub.keys():\n",
        "        for i in range(len(worddic_sub[k])):\n",
        "            words_docs[k].append(worddic_sub[k][i][0])\n",
        "    # keep onlt the unique docs per word      \n",
        "    for k in words_docs:\n",
        "        words_docs[k] = set(words_docs[k])\n",
        "    # combination of all docs\n",
        "    comboindex = []\n",
        "    for k in words_docs:\n",
        "        comboindex += words_docs[k]\n",
        "    # count the number of each doc (from 0 to max number of search words)\n",
        "    combocount = Counter(comboindex) \n",
        "    # divide by number of search words (to get in range from [0,1])\n",
        "    for key in combocount:\n",
        "        combocount[key] = combocount[key] / numwords\n",
        "    # sort from highest to lowest\n",
        "    combocount_order = sorted(combocount.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    ## metric closedic: \n",
        "    # check on words appearing in same order as in search\n",
        "    fdic_order = 0 # initialization (in case of a single search word)\n",
        "    if len(words) > 1:\n",
        "        # list with docs with a search word        \n",
        "        x = [index[0] for record in [worddic[z] for z in words] for index in record]\n",
        "        # list with docs with more than one search word\n",
        "        # y = sorted(list(set([i for i in x if x.count(i) > 1])))\n",
        "        counts = np.bincount(x)\n",
        "        y = list(np.where([counts>1])[1])\n",
        "\n",
        "        # dictionary of documents and all positions \n",
        "        # (for docs with more than one search word in it)\n",
        "        closedic = {}\n",
        "        y = set(y) # speed up processing\n",
        "        for wordbig in [worddic[x] for x in words]:\n",
        "            for record in wordbig:\n",
        "                if record[0] in y:\n",
        "                    index = record[0]\n",
        "                    positions = record[1]\n",
        "                    try:\n",
        "                        closedic[index].append(positions)\n",
        "                    except:\n",
        "                        closedic[index] = []\n",
        "                        closedic[index].append(positions)\n",
        "    \n",
        "        ## metric fdic: \n",
        "        # number of times search words appear in a doc in descending order\n",
        "        fdic = {}\n",
        "        # fdic_order = []\n",
        "        for index in y: # list with docs with more than one search word\n",
        "            x = 0 \n",
        "            csum = []            \n",
        "            for seqlist in closedic[index]:\n",
        "                while x > 0:\n",
        "                    secondlist = seqlist # second word positions\n",
        "                    x = 0\n",
        "                    # first and second word next to each other (in same order)\n",
        "                    sol = [1 for i in firstlist if i + 1 in secondlist]\n",
        "                    csum.append(sol)\n",
        "                    fsum = [item for sublist in csum for item in sublist] \n",
        "                    fsum = sum(fsum) \n",
        "                    fdic[index] = fsum\n",
        "\n",
        "                while x == 0:\n",
        "                    firstlist = seqlist # first word positions \n",
        "                    x += 1 \n",
        "        fdic_order = sorted(fdic.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    ## keep only docs that contains all must_search_words\n",
        "    if must_have_word != None and numwords > 1:\n",
        "        # check if word is in dictionary\n",
        "        if must_have_word.lower() not in word2idx:\n",
        "            print(\"\\nMust-have-word not found in dictionary\")\n",
        "        else:\n",
        "            # lower case must_have_word\n",
        "            must_have_word = word2idx[must_have_word.lower()] \n",
        "            # get list of all docs with a must have word \n",
        "            must_have_docs = set([doc[0] for doc in worddic_sub[must_have_word]])\n",
        "            # update the score metrics containing only docs with the must have search word\n",
        "            fullcount_order = [list_of_list for list_of_list in fullcount_order\\\n",
        "                               if list_of_list[0] in must_have_docs]\n",
        "            combocount_order = [list_of_list for list_of_list in combocount_order\\\n",
        "                                if list_of_list[0] in must_have_docs]    \n",
        "            fullidf_order = [list_of_list for list_of_list in fullidf_order\\\n",
        "                             if list_of_list[0] in must_have_docs]\n",
        "            fdic_order = [list_of_list for list_of_list in fdic_order\\\n",
        "                          if list_of_list[0] in must_have_docs]\n",
        "    \n",
        "    \n",
        "    ## idx2word all words (transform words again in characters instead of numbers)\n",
        "    words = [idx2word[word] for word in words]\n",
        "\n",
        "    return (searchsentence, words, fullcount_order, combocount_order, fullidf_order, fdic_order)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoZSEqMkMgkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN-SHnJj7r8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sqTfbCIFCUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def rank(term, must_have_word=None):\n",
        "\n",
        "    # get results from search\n",
        "    results = search(term, must_have_word)\n",
        "    # get metrics\n",
        "    # search words found in dictionary:\n",
        "    search_words = results[1] \n",
        "    # number of search words found in dictionary:\n",
        "    num_search_words = len(results[1]) \n",
        "    # number of search words (as in dict) in each doc (in descending order):\n",
        "    num_score = results[2] \n",
        "    # percentage of search words (as in dict) in each doc (in descending order):\n",
        "    per_score = results[3]\n",
        "    # sum of tfidf of search words in each doc (in ascending order):\n",
        "    tfscore = results[4] \n",
        "    # fidc order:\n",
        "    order_score = results[5] \n",
        "\n",
        "    # list of documents in order of relevance\n",
        "    final_candidates = []\n",
        "\n",
        "\n",
        "    ## no search term(s) not found\n",
        "    if num_search_words == 0:\n",
        "        print('Search term(s) not found')\n",
        "\n",
        "\n",
        "    ## single term searched (as in dict): return the following 5 scores\n",
        "    if num_search_words == 1:\n",
        "        # document numbers:\n",
        "        num_score_list = [l[0] for l in num_score] \n",
        "        # take max 3 documents from num_score:\n",
        "        num_score_list = num_score_list[:3] \n",
        "        # add the best percentage score:\n",
        "        num_score_list.append(per_score[0][0]) \n",
        "        # add the best tfidf score\n",
        "        num_score_list.append(tfscore[0][0]) \n",
        "        # remove duplicate document numbers\n",
        "        final_candidates = list(set(num_score_list)) \n",
        "\n",
        "\n",
        "    ## more than one search word (and found in dictionary)\n",
        "    # ranking is based on an intelligent commbination of the scores\n",
        "    if num_search_words > 1:\n",
        "\n",
        "        ## set up a dataframe with scores of size all documents (initalized with 0)\n",
        "        total_number_of_docs = len(df)\n",
        "        doc_score_columns = ['num_score', 'per_score', 'tf_score', 'order_score']\n",
        "        doc_score = pd.DataFrame(0, index=np.arange(total_number_of_docs), columns=doc_score_columns)\n",
        "        \n",
        "        # plot a score in a daraframe (index is doc number)\n",
        "        def doc_plot(type_score):\n",
        "            score_doc = [0]*total_number_of_docs\n",
        "            for i in range(len(type_score)):\n",
        "                x = type_score[i][0] # document number\n",
        "                score_doc[x] = float(type_score[i][1]) # score\n",
        "            return score_doc\n",
        "        \n",
        "        # Fill-in for each doc the score\n",
        "        doc_score.num_score = doc_plot(num_score)\n",
        "        doc_score.per_score = doc_plot(per_score)\n",
        "        doc_score.tf_score = doc_plot(tfscore)\n",
        "        doc_score.order_score = doc_plot(order_score)\n",
        "        \n",
        "        # Normalize (to the max)\n",
        "        maximum = max(doc_score.num_score)\n",
        "        doc_score.num_score = [float(i)/maximum for i in doc_score.num_score]\n",
        "        \n",
        "        # keep per_score (percentage of search words in document) as it is (between 0 and 1)\n",
        "        # doc_score.per_score = [float(i)/sum(doc_score.per_score) for i in doc_score.per_score]\n",
        "        \n",
        "        # TODO check tf score keep it as it is (not normalizing)\n",
        "        maximum = max(doc_score.tf_score)        \n",
        "        doc_score.tf_score = [float(i)/maximum for i in doc_score.tf_score]\n",
        "        \n",
        "        maximum = max(doc_score.order_score)   \n",
        "        doc_score.order_score = [float(i)/maximum for i in doc_score.order_score]\n",
        "        \n",
        "        # sum all scores to get a Grand Score\n",
        "        doc_score['sum'] = (doc_score.num_score +\\\n",
        "                            doc_score.per_score +\\\n",
        "                            doc_score.tf_score +\\\n",
        "                            doc_score.order_score)\n",
        "        # keep only the values with a sum > 0\n",
        "        doc_score = doc_score[doc_score['sum'] > 0]\n",
        "        \n",
        "        # get the docs (i.e index) of the hightest raknkimng from hgh to low\n",
        "        final_candidates = doc_score.sort_values('sum', ascending=False).index\n",
        "    \n",
        "        # keep 15 top candidates\n",
        "        final_candidates = final_candidates[:15]        \n",
        "\n",
        "    # print final candidates\n",
        "    print('\\nFound search words:', results[1])\n",
        "\n",
        "    # top results: sentences with search words, paper ID (and document number), authors and abstract\n",
        "    df_results = pd.DataFrame(columns=\\\n",
        "              [\"doc_id\",\"doi\", \"publish_time\",\"Document_no\",\"authors\",\"url\",\"title\",\"Sentences\", \"Search_words\"])\n",
        "              # ['doc_id', 'Paper_id', 'Document_no', 'Authors', 'Abstract', 'Sentences', 'Search_words'])\n",
        "    for index, results in enumerate(final_candidates):\n",
        "        df_results.loc[index, 'doc_id'] = df.doc_id[results]\n",
        "        df_results.loc[index, 'doi'] = df.doi[results]\n",
        "        df_results.loc[index, 'publish_time'] = df.publish_time[results]\n",
        "        df_results.loc[index, 'Document_no'] = results\n",
        "        df_results.loc[index, 'authors'] = df.authors[results]\n",
        "        df_results.loc[index, 'url'] = df.url[results]\n",
        "        df_results.loc[index, 'title'] = df.title[results]\n",
        "\n",
        "        # get sentences with search words and all search words in the specific document\n",
        "        sentence_index, search_words_found = search_sentence(results, search_words)\n",
        "        # all sentences with search words\n",
        "        df_results.loc[index, 'Sentences'] = sentence_index\n",
        "        # all search words (also multiple instances) \n",
        "        df_results.loc[index, 'Search_words'] = search_words_found\n",
        "          \n",
        "    return final_candidates, df_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6AZYVGLFMVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def search_sentence(doc_number, search_words):\n",
        "    sentence_index = [] # all sentences with search words \n",
        "    search_words_found = [] # all found search words\n",
        "    \n",
        "    for sentence in sentences[doc_number]:\n",
        "        # keep characters as in worddic, lowercase, split in words and lemmatize\n",
        "        sentence_temp = re.sub(r'[^a-zA-Z]', ' ', sentence)\n",
        "        sentence_temp = sentence_temp.lower() # lowercase\n",
        "        sentence_temp = sentence_temp.split() # split in different words\n",
        "        \n",
        "        # all sentences with search words\n",
        "        for search_word in search_words:\n",
        "            if search_word in sentence_temp:\n",
        "                sentence_index.append(sentence)\n",
        "                break\n",
        "        # all search words (also multiple instances)\n",
        "        for search_word in search_words:\n",
        "            if search_word in sentence_temp:\n",
        "                search_words_found.append(search_word)\n",
        "        # [search_words_found.append(search_word) for search_word in search_words if search_word in sentence_temp]\n",
        "            \n",
        "    return sentence_index, search_words_found "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9w18c7QGXGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# return this text with the highlighted words\n",
        "def highlight_words(text, words, color):\n",
        "    \n",
        "    # color set\n",
        "    color_set = {'red': '\\033[31m', 'green': '\\033[32m','blue': '\\033[34m','reset': '\\033[39m'}\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    \n",
        "    # wrap words in color\n",
        "    for word in words:\n",
        "        # text_lower = text.lower() # lowercase words\n",
        "        text_temp = [re.sub(r'[^a-zA-Z]', '', word) for word in text]\n",
        "        # lemmatize\n",
        "        text_temp = [lemmatizer.lemmatize(word) for word in text_temp] \n",
        "        # idxs = [i for i, x in enumerate(text) if x.lower() == word]\n",
        "        idxs = [i for i, x in enumerate(text_temp) if x.lower() == word]\n",
        "        for i in idxs:\n",
        "            text[i] = color_set[color] + text[i] + color_set['reset']\n",
        "            \n",
        "    # join the list back into a string and print\n",
        "    text_highlighted = ' '.join(text)\n",
        "    return(text_highlighted)\n",
        "         \n",
        "\n",
        "## Main function of printing the papers in ranked order\n",
        "# Select per document: \n",
        "# - top_n: number of top n papers to be displayed\n",
        "# - show_sentences: display the sentences which contains search words\n",
        "# - show_wordcloud: dsiplay a cloud word of these sentences\n",
        "def print_ranked_papers(ranked_result, top_n=3, show_abstract=True, show_sentences=True):\n",
        "\n",
        "    # Print top n result (with max number of documents from ranked_result)\n",
        "    for index in range(min(top_n, len(ranked_result))):    \n",
        "       \n",
        "        ## Preparation\n",
        "        # join all sentences and seperate by a return\n",
        "        text_sentences = '\\n'.join(ranked_result.Sentences[index])\n",
        "        # spit in seperate words \n",
        "        text_sentences_split = text_sentences.split()\n",
        "        # list of search words\n",
        "        search_words = list(set(ranked_result.Search_words[index]))\n",
        "\n",
        "        \n",
        "        ## Print most important items per document (paper)\n",
        "        # and in case of 'nan' write 'not available'\n",
        "        \n",
        "        # ranking number and title\n",
        "        if pd.isnull(ranked_result.title[index]):\n",
        "            print('\\n\\nRESULT {}:'. format(index+1), 'Title not available')\n",
        "        else: \n",
        "                # Print Result from 1 and not 0\n",
        "                print('\\n\\nRESULT {}:'. format(index+1), ranked_result.title[index]) \n",
        "    \n",
        "        # generate cloud word\n",
        "        # if len(text_sentences_split)>0:\n",
        "        #   wordcloud = WordCloud()\n",
        "        #   img = wordcloud.generate_from_text(' '.join(text_sentences_split))\n",
        "        #   plt.imshow(img)\n",
        "        #   plt.axis('off')\n",
        "        #   plt.show()   \n",
        "\n",
        "       # [\"doc_id\",\"doi\", \"publish_time\",\"Document_no\",\"authors\",\"url\",\"title\",\"Sentences\", \"Search_words\"])\n",
        "        \n",
        "        # count all search word in document and present them from highest to lowest  \n",
        "        dict_search_words =\\\n",
        "            dict(Counter(ranked_result.Search_words.iloc[index]).most_common())\n",
        "        print('\\nI Number of search words in paper:')\n",
        "        for k,v in dict_search_words.items():\n",
        "            print('- {}:'.format(k), v)\n",
        "            \n",
        "        # paper id and document number\n",
        "        print('\\nII Paper ID:', ranked_result.doc_id[index], \n",
        "              '(Document no: {})'. format(ranked_result.Document_no[index]))\n",
        "        \n",
        "        # authors\n",
        "        if pd.isnull(ranked_result.authors[index]):\n",
        "            print('\\nIII Authors:', 'Authors not available')\n",
        "        else:\n",
        "            print('\\nIII Authors:', ranked_result.authors[index])\n",
        "        print('\\n')\n",
        "        \n",
        "        # abstract\n",
        "        # if show_abstract == True:\n",
        "        #     if pd.isnull(ranked_result.Abstract[index]):\n",
        "        #         print('Abstract not available')\n",
        "        #     else: \n",
        "        #         # split abstract in seperate words\n",
        "        #         abstract_sentences_split = ranked_result.Abstract[index].split()\n",
        "        #         # highlight the search words in red\n",
        "        #         print(highlight_words(abstract_sentences_split, search_words, 'red'))\n",
        "              \n",
        "        ## show sentences with search words in green\n",
        "        if show_sentences == True:\n",
        "            print('\\nIV Sentences in paper containing search words:\\n')\n",
        "            print(highlight_words(text_sentences_split, search_words,'green'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yK0m-uzexS2",
        "colab_type": "code",
        "outputId": "fa0b3164-86c0-45f7-df53-024832186c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "must_have_word = 'incubation period, day'\n",
        "search_example = 'What is incubation period'\n",
        "papers, rank_result = rank(search_example,must_have_word)\n",
        "# papers, rank_result = rank(search_example)\n",
        "\n",
        "# Print final candidates\n",
        "print('Top 10 papers (document numbers):', papers)\n",
        "\n",
        "# Print results\n",
        "print_ranked_papers(rank_result, top_n=10, show_abstract=False, show_sentences=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Must-have-word not found in dictionary\n",
            "\n",
            "Found search words: ['period', 'incubation']\n",
            "Top 10 papers (document numbers): Int64Index([2198,  366, 1164, 1560,  840,  545,  629, 1990, 1961,  986, 1745,\n",
            "            2070, 1177, 2274, 2182],\n",
            "           dtype='int64')\n",
            "\n",
            "\n",
            "RESULT 1: Citation: Development of an Immunochromatographic Strip for Rapid Detection of Canine Adenovirus\n",
            "\n",
            "I Number of search words in paper:\n",
            "\n",
            "II Paper ID: 1d105041da2d3a6e6e16cd0dce1f0a672066d407 (Document no: 2198)\n",
            "\n",
            "III Authors: Wang, Shujie; Wen, Yongjun; An, Tongqing; Duan, Guixin; Sun, MingXia; Ge, Jinying; Li, Xi; Yang, Kongbin; Cai, Xuehui\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULT 2: Mucin 4 Protects Female Mice from Coronavirus Pathogenesis\n",
            "\n",
            "I Number of search words in paper:\n",
            "\n",
            "II Paper ID: 70c7260fc0702bd4a554421ac68972c991313e56 (Document no: 366)\n",
            "\n",
            "III Authors: Jessica A. Plante; Kenneth S. Plante; Lisa E. Gralinski; Anne Beall; Martin T. Ferris; Daniel Bottomly; Richard Green; Shannon K. McWeeney; Mark T. Heise; Ralph S. Baric; Vineet D. Menachery\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULT 3: Bioaerosols Play a Major Role in the Nasopharyngeal Microbiota Content in Agricultural Environment\n",
            "\n",
            "I Number of search words in paper:\n",
            "\n",
            "II Paper ID: 98db52b9c67490372213988b16fbd56918e3d52f (Document no: 1164)\n",
            "\n",
            "III Authors: Mbareche, Hamza; Veillette, Marc; Pilote, Jonathan; Ltourneau, Valrie; Duchaine, Caroline\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULT 4: Epidemiological research priorities for public health control of the ongoing global novel coronavirus (2019- nCoV) outbreak\n",
            "\n",
            "I Number of search words in paper:\n",
            "\n",
            "II Paper ID: 90de2d957e1960b948b8c38c9877f9eca983f9eb (Document no: 1560)\n",
            "\n",
            "III Authors: Cowling, Benjamin J; Leung, Gabriel M\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULT 5: Clinical and Laboratory Profiles of 75 Hospitalized Patients with Novel\n",
            "\n",
            "I Number of search words in paper:\n",
            "\n",
            "II Paper ID: c8437a45bfb84fb206fe03fd18d28858bae32651 (Document no: 840)\n",
            "\n",
            "III Authors: Zonghao Zhao; Jiajia Xie; Ming Yin; Yun Yang; Hongliang He; Tengchuan Jin; Wenting Li; Xiaowu Zhu; Jing Xu; Changcheng Zhao; Lei Li; Yi Li; Hylemariam Mihiretie Mengist; Ayesha Zahid; Ziqin Yao; Chengchao Ding; Yingjie Qi; Yong Gao; Xiaoling Ma\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULT 6: Geometric coupling of helicoidal ramps and curvature-inducing proteins in organelle membranes\n",
            "\n",
            "I Number of search words in paper:\n",
            "\n",
            "II Paper ID: 681c5099b2afb08fe5cb2f58d92e9c6e4873bec2 (Document no: 545)\n",
            "\n",
            "III Authors: Morgan Chabanon; Padmini Rangamani\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULT 7: A Generalized Discrete Dynamic Model for Human Epidemics\n",
            "\n",
            "I Number of search words in paper:\n",
            "- period: 7\n",
            "- incubation: 7\n",
            "\n",
            "II Paper ID: 4d1090e239baa1ea54af521f4b8bd8b5ab51fd35 (Document no: 629)\n",
            "\n",
            "III Authors: Wenjun Zhang; Zeliang Chen; Yi Lu; Zhongmin Guo; Yanhong Qi; Guoling Wang; Jiahai Lu\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "Furthermore, the important parameters such as \u001b[32mincubation\u001b[39m \u001b[32mperiod,\u001b[39m hospitalization term, etc., were not included in such explanatory models and most of the other models (Chen et al., 2020a) . https://doi.org/10.1101/2020.02.11.944728 doi: bioRxiv preprint TCC decreases dramatically with mean \u001b[32mincubation\u001b[39m \u001b[32mperiod\u001b[39m (c) when c is less than a certain value while it slowly increases with mean \u001b[32mincubation\u001b[39m \u001b[32mperiod\u001b[39m (c) if c is greater than the value (Fig. One day's decrease in mean \u001b[32mincubation\u001b[39m \u001b[32mperiod\u001b[39m (c) will lead to a decline in TCC by 1817%. 4 Effect of the change of mean \u001b[32mincubation\u001b[39m \u001b[32mperiod\u001b[39m (c). Mean \u001b[32mincubation\u001b[39m \u001b[32mperiod\u001b[39m is another key factor in determining disease epidemics. In a certain range, a short \u001b[32mincubation\u001b[39m \u001b[32mperiod\u001b[39m may lead to the rapid development and serious outcome of epidemic disease. On the other hand, some parameter, e.g., \u001b[32mincubation\u001b[39m \u001b[32mperiod\u001b[39m falls in an interval rather than a deterministic value.\n",
            "\n",
            "\n",
            "RESULT 8: Neuraminidase activity and specificity of influenza A virus are influenced by haemagglutinin-receptor binding\n",
            "\n",
            "I Number of search words in paper:\n",
            "- incubation: 3\n",
            "\n",
            "II Paper ID: 2c3fc2c522ed72e939effed158240c1c971fbdad (Document no: 1990)\n",
            "\n",
            "III Authors: Lai, Jimmy Chun Cheong; Karunarathna, Herath M. T. K.; Wong, Ho Him; Peiris, Joseph S. M.; Nicholls, John M.\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "Human H1N1pdm and avian H9N2 (HK/G1/97) influenza viruses were pre-treated with TPCK-trypsin followed by \u001b[32mincubation\u001b[39m in acetate buffer (at pH 5.0 and 6.5). The ability to agglutinate TRBC was lost in both H1N1pdm and H9N2 viruses after pre-incubation at pH 5.0, compared to the control treatment at pH 6.5, indicating a loss of HA-receptorbinding property. The methodology of low pH \u001b[32mincubation\u001b[39m for HA-inactivation may also become a useful tool for future study of NA on the whole virus without an interference from the HA functions.\n",
            "\n",
            "\n",
            "RESULT 9: Three asymptomatic animal infection models of hemorrhagic fever with renal syndrome caused by hantaviruses\n",
            "\n",
            "I Number of search words in paper:\n",
            "- incubation: 1\n",
            "- period: 1\n",
            "\n",
            "II Paper ID: 02a009e42054081b441d0f4b203679c4b0cae38d (Document no: 1961)\n",
            "\n",
            "III Authors: Perley, Casey C.; Brocato, Rebecca L.; Kwilas, Steven A.; Daye, Sharon; Moreau, Alicia; Nichols, Donald K.; Wetzel, Kelly S.; Shamblin, Joshua; Hooper, Jay W.\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "Following this \u001b[32mincubation,\u001b[39m 50 l was inoculated onto Vero cell monolayers in a clear bottom, black-walled 96-well plate in duplicate. However, no prolonged signs of clinical kidney failure were observed: blood urea nitrogen and creatinine levels did not dramatically increase over the five week study \u001b[32mperiod.\u001b[39m\n",
            "\n",
            "\n",
            "RESULT 10: Immunogenicity of Different Forms of Middle East Respiratory Syndrome S Glycoprotein\n",
            "\n",
            "I Number of search words in paper:\n",
            "\n",
            "II Paper ID: 24beb94c313a07be6b0c745a6ca8a3d810618506 (Document no: 986)\n",
            "\n",
            "III Authors: Ozharovskaia, T. A.; Zubkova, O. V.; Dolzhikova, I. V.; Gromova, A. S.; Grousova, D. M.; Tukhvatulin, A. I.; Popova, O.; Shcheblyakov, D. V.; Scherbinin, D. N.; Dzharullaeva, A. S.; Erokhova, A. S.; Shmarov, M. M.; Loginova, S. Y.; Borisevich, S. V.; Naroditsky, B. S.; Logunov, D. Y.; Gintsburg, A. L.\n",
            "\n",
            "\n",
            "\n",
            "IV Sentences in paper containing search words:\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHz1jscfm4yD",
        "colab_type": "code",
        "outputId": "0c6d60fb-fe6d-4c72-a680-51f50fd44d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>doi</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>authors</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>text_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>43d2d4072d40a3964ea330342b9222376d700531</td>\n",
              "      <td>10.3201/eid2509.181937</td>\n",
              "      <td>2019-09-10</td>\n",
              "      <td>Jing, Shuping; Zhang, Jing; Cao, Mengchan; Liu...</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>Lassa fever in travelers from West Africa</td>\n",
              "      <td>We identified a case of fatal acute respirator...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cf692fd14d468eaca2c460132802463f0747411e</td>\n",
              "      <td>10.3201/eid2503.171702</td>\n",
              "      <td>2019-03-10</td>\n",
              "      <td>Farag, Elmoubasher Abu Baker; Nour, Mohamed; E...</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>Survey on Implementation of One Health Approac...</td>\n",
              "      <td>H uman infections with Middle East respiratory...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4f2102dfde801cb0cab691992bbc61cbd98b1ca5</td>\n",
              "      <td>10.3201/eid2502.180798</td>\n",
              "      <td>2019-02-10</td>\n",
              "      <td>Rampling, Tommy; Page, Mark; Horby, Peter</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Although several hundred centers contribute to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45e91042ac20319b849f63a7dd00dda7d11de638</td>\n",
              "      <td>10.3201/eid2601.ac2601</td>\n",
              "      <td>2020-01-10</td>\n",
              "      <td>Breedlove, Byron</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>Hunters Searching among Starry Nights and at t...</td>\n",
              "      <td>Director of Astrobiology at Columbia Universit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d84f1c9036052ca0d9f8216cac4c5118ad8cea19</td>\n",
              "      <td>10.3201/eid2512.191002</td>\n",
              "      <td>2019-12-10</td>\n",
              "      <td>Stoian, Ana M.M.; Zimmerman, Jeff; Ji, Ju; Hef...</td>\n",
              "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>However, half-life calculations were based on ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     doc_id  ...                                          text_body\n",
              "0  43d2d4072d40a3964ea330342b9222376d700531  ...  We identified a case of fatal acute respirator...\n",
              "1  cf692fd14d468eaca2c460132802463f0747411e  ...  H uman infections with Middle East respiratory...\n",
              "2  4f2102dfde801cb0cab691992bbc61cbd98b1ca5  ...  Although several hundred centers contribute to...\n",
              "3  45e91042ac20319b849f63a7dd00dda7d11de638  ...  Director of Astrobiology at Columbia Universit...\n",
              "4  d84f1c9036052ca0d9f8216cac4c5118ad8cea19  ...  However, half-life calculations were based on ...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmzYmOc3mjPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89q8plkikxjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}